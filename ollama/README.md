# Ollama Tool

A Rust-based CLI tool for managing Ollama models. It fetches the latest available models from Ollama's online library, enables downloading specific models, and provides an interface to run them interactively.

## Prerequisites

- Rust installed (via setup script)
- Ollama installed and running (`brew services start ollama`)

## Installation

Clone the repository and build the tool:

```bash
git clone https://github.com/bniladridas/github-dotfiles.git
cd github-dotfiles/ollama
cargo build --release
```

### Quick Start

For a one-click setup and run with the lightest model:

```bash
./quick-start.sh
```

This script checks Ollama, builds the tool, pulls `tinyllama`, and runs it.

## Usage

Use the provided script to automatically build and run the tool.

<!-- prettier-ignore -->
```bash
# Show help
./target/release/github-dotfiles-ollama --help

# List all available models from Ollama's library
./target/release/github-dotfiles-ollama list

# Download a model (e.g., tinyllama)
./target/release/github-dotfiles-ollama pull tinyllama

# Run a downloaded model interactively
./target/release/github-dotfiles-ollama run tinyllama

# Generate a response with custom system prompt
./target/release/github-dotfiles-ollama generate tinyllama "Hello" --system "You are a helpful assistant."
```

The script builds the tool if needed and passes arguments to it.

### Commands

- `list`: Fetches and displays available models from ollama.ai/library
- `installed`: Lists locally installed models
- `pull <model>`: Downloads the specified model using Ollama CLI
- `run <model>`: Launches the model in interactive chat mode
- `generate <model> <prompt> [--system <prompt>]`: Generates a response using the API with custom system prompt
- `remove <model>`: Removes the specified model from local storage

## Handling Errors

If Ollama is not running, `pull` and `run` commands will fail with an error message. Ensure Ollama is started with `brew services start ollama`.

Common errors:

| Error | Cause | Solution |
|-------|-------|----------|
| "Ollama command failed" | Ollama not running | Start Ollama service |
| "Model not found" | Invalid model name | Check spelling or list available models |
| Network error | Internet connection | Retry or check connection |

## Advanced Usage

### Custom Model Selection

Use `list` to browse models, then `pull` specific ones. Models are pulled in the background and may take time for large models.

### Integration with Scripts

The tool can be integrated into automation scripts:

```bash
#!/bin/bash
MODEL="llama2"
./target/release/github-dotfiles-ollama pull $MODEL
./target/release/github-dotfiles-ollama run $MODEL
```

## Features

- **Model Discovery**: Dynamically fetches model list from ollama.ai/library
- **Seamless Download**: Integrates with Ollama CLI for reliable model pulling
- **Interactive Running**: Launches models in chat mode
- **Error Handling**: Provides clear feedback for failed operations

## Example Workflow

1. List models: `./target/release/github-dotfiles-ollama list`
2. Choose a model (e.g., `llama2`)
3. Pull it: `./target/release/github-dotfiles-ollama pull llama2`
4. Run it: `./target/release/github-dotfiles-ollama run llama2`